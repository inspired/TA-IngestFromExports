# BEGIN Use case 1: Ingest from Exporttool

[splunk_exporttool]
INDEXED_EXTRACTIONS = CSV
TIME_FORMAT = %s
TIME_PREFIX = ^
MAX_TIMESTAMP_LOOKAHEAD = 10
TRANSFORMS-csv_field_fix = csv_field_fix
SHOULD_LINEMERGE = false
MAX_EVENTS = 0
TRUNCATE = 0
MAX_DAYS_AGO = 10951
MAX_DAYS_HENCE = 10951
MAX_DIFF_SECS_AGO = 2147483646
MAX_DIFF_SECS_HENCE = 2147483646

# END Use case 1: Ingest from Exporttool

# BEGIN Use case 2: Ingest from Ingest Actions

[source::....zstd?(.\d+)?]
unarchive_cmd = zstd --stdout -d
sourcetype = preprocess-zstd
NO_BINARY_CHECK = true

[preprocess-zstd]
invalid_cause = archive
is_valid = False
LEARN_MODEL = false

# NDJSON from Ingest Actions
[rfs_input]
INDEXED_EXTRACTIONS = JSON
TRUNCATE = 0
TRANSFORMS-rfs_ndjson_rewrite = rfs_ndjson_rewrite, rfs_ndjson_write_indexed_fields, rfs_ndjson_null_indexed_fields
# Use the below TRANSFORMS if you want to keep the original index. In this case, define a lastChanceIndex = <index name> in indexes.conf as well to capture events going to indexes that do not exist.
#TRANSFORMS-rfs_ndjson_rewrite = rfs_ndjson_rewrite_with_index, rfs_ndjson_write_indexed_fields, rfs_ndjson_null_indexed_fields
LINE_BREAKER = ([\r\n]+)
SHOULD_LINEMERGE = false
disabled = false
pulldown_type = true

# END Use case 2: Ingest from Ingest Actions

